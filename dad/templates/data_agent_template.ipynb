{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "da14421b-b20c-49d8-9ed6-53eedfaad93c",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Create Data Agent\n",
        "\n",
        "This notebook utilises the Fabric Data Agent SDK to create configure and publish Fabric Data Agents via Code.\n",
        "\n",
        "Please refers to examples by Microsoft for more information.\n",
        "\n",
        "- https://github.com/microsoft/fabric-samples/tree/main/docs-samples/data-science/data-agent-sdk\n",
        "\n",
        "#### Structure\n",
        "0. Imports and Configuring Default Lakehouse Connection - this may be removed later though\n",
        "1. Initialising and Configuring Agent Instructions\n",
        "2. Connecting Data Sources\n",
        "3. Configuring Data Source Instructions and Example Queries\n",
        "4. Publishing Agent\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dc7a82d1",
      "metadata": {},
      "source": [
        "## Imports and Configuring Default Lakehouse Connection\n",
        "\n",
        "Not sure if this is fully necessary yet... requires a bit more testing but I think this will be removed further along the line"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d45e5245",
      "metadata": {},
      "outputs": [],
      "source": [
        "%%configure\n",
        "{\n",
        "    \"defaultLakehouse\": {  \n",
        "        \"name\": \"DataAgentDefaultLH\",\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a265275-e023-403d-840e-0c259b5be32b",
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        }
      },
      "outputs": [],
      "source": [
        "# Importing tools from Fabric Data Agent SDK\n",
        "from fabric.dataagent.client import (\n",
        "    FabricDataAgentManagement,\n",
        "    create_data_agent,\n",
        "    delete_data_agent,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5196965f-c967-4ff0-bd58-ab50ca387ddb",
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Initialising and Configuring Agent Instructions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c6160bdc-df77-481a-811b-531d7c3c637f",
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        }
      },
      "outputs": [],
      "source": [
        "# Configuration\n",
        "data_agent_name = \"data-agent-name\"\n",
        "data_sources = {\n",
        "    \"lakehouse\": [\"DataAgentDefaultLH\"] # I will add more example data sources later in the future but for now its just Lakehouses cause idk the other type names...\n",
        "    }\n",
        "lakehouse_table_names = {\n",
        "    \"DataAgentDefaultLH\": [\"projects\", \"employees\", \"invoices\", \"clients\", \"project_tasks\"]\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f2bb493-cf60-4738-ba2f-c36f9a248e8b",
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        }
      },
      "outputs": [],
      "source": [
        "# Create Data Agent\n",
        "data_agent = create_data_agent(data_agent_name)\n",
        "data_agent = FabricDataAgentManagement(data_agent_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b8b3aa7-ec77-499c-8828-fa4363cc4f6e",
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        }
      },
      "outputs": [],
      "source": [
        "# Update or specify (if done for the first time) the data agent instructions. Below is a sample...\n",
        "data_agent.update_configuration(\n",
        "    instructions= \"\"\"\n",
        "    You are a helpful, precise, and context-aware data agent that assists users in querying project-related data for an engineering consulting company.\n",
        "\n",
        "    Your goal is to translate natural-language questions into valid SQL queries that retrieve relevant information from the available data tables.  \n",
        "    The data represents ongoing and past engineering consulting projects, their clients, staff, tasks, and invoices.\n",
        "\n",
        "    DATA TABLES\n",
        "    ------------\n",
        "    - projects\n",
        "    - clients\n",
        "    - employees\n",
        "    - project_tasks\n",
        "    - invoices\n",
        "\n",
        "    JOIN KEYS\n",
        "    ----------\n",
        "    projects.client_id = clients.client_id  \n",
        "    projects.project_manager_id = employees.employee_id  \n",
        "    projects.project_id = project_tasks.project_id  \n",
        "    projects.project_id = invoices.project_id  \n",
        "    project_tasks.assigned_to = employees.employee_id  \n",
        "\n",
        "    BEHAVIOR GUIDELINES\n",
        "    --------------------\n",
        "    • Always generate ANSI-SQL syntax (compatible with SQLite or standard SQL engines).  \n",
        "    • Query only existing columns and relationships. Ask clarifying questions when uncertain.  \n",
        "    • Join only when needed (e.g., when client or manager info is requested).  \n",
        "    • Use aggregation functions (SUM, COUNT, AVG) and GROUP BY appropriately.  \n",
        "    • Summarize clearly when responding in natural language.  \n",
        "    • Use consistent table aliases:  \n",
        "    - p for projects  \n",
        "    - c for clients  \n",
        "    - e for employees  \n",
        "    - t for project_tasks  \n",
        "    - i for invoices  \n",
        "\n",
        "    EXAMPLE BEHAVIOR\n",
        "    -----------------\n",
        "    User: “Show me all projects managed by Ben Li with pending invoices.”  \n",
        "    → Join projects + employees + invoices.  \n",
        "    → Filter e.name = 'Ben Li' AND i.status = 'Pending'.  \n",
        "    → Return project name, client, and invoice amount.\n",
        "    \"\"\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f55a8334",
      "metadata": {},
      "source": [
        "## Connecting Data Sources"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c55f3b6f-f413-4e5e-a822-4e3a02bbac96",
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        }
      },
      "outputs": [],
      "source": [
        "# Run this if you developing within the Fabric Portal and can actively retrieve the list of data sources available in the workspace\n",
        "\n",
        "def safe_get_datasources(): # add_datasource calls get_datasources, which doesn't handle None types.... This is a modified function which handles this\n",
        "    config = data_agent._client.get_configuration()\n",
        "    data_sources = config.value.get(\"dataSources\") or []\n",
        "    return [data_agent._client.get_datasource(ds[\"id\"]) for ds in data_sources]\n",
        "\n",
        "data_agent._client.get_datasources = safe_get_datasources"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "669f5d26",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Adding all lakehouse data sources\n",
        "for lakehouse_name in data_sources['lakehouse']:\n",
        "    data_agent.add_datasource(lakehouse_name, type=\"lakehouse\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2bc5ebdf",
      "metadata": {},
      "source": [
        "## Configuring Data Source Instructions and Example Queries\n",
        "\n",
        "In this section, we setup the data source instructions and examples queries for **one** lakehouse connection but if there are others data source connections, you must repeat this setup for each individual data source."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc0aef21-0505-4630-a36a-1f2546496d3f",
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        }
      },
      "outputs": [],
      "source": [
        "# Get the first data source\n",
        "datasource = data_agent.get_datasources()[0] # change to 1 for the 2nd data source and 2 for the 3rd and so on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5127a22-c2f8-418d-b0c2-70b523627309",
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        }
      },
      "outputs": [],
      "source": [
        "for table_name in lakehouse_table_names[\"DataAgentDefaultLH\"]:\n",
        "    datasource.select(\"dbo\", table_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5284ec27-ba3b-4a3f-9073-aede3f472ebf",
      "metadata": {
        "jupyter": {
          "outputs_hidden": true
        },
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        }
      },
      "outputs": [],
      "source": [
        "# # Run this if you developing within the Fabric Portal and want to see the structure of the data source.\n",
        "# datasource.pretty_print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3dfe4fa5-c744-417b-acfd-0ea071c10ae3",
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        }
      },
      "outputs": [],
      "source": [
        "# Update or specify (if done for the first time) the data source instructions. Below is a sample...\n",
        "datasource.update_configuration(instructions=\"\"\"\n",
        "This dataset represents the projects, clients, staff, and financial operations of an engineering consulting company.  \n",
        "It supports queries about project progress, management, clients, staffing, and billing.\n",
        "\n",
        "TABLE 1 - projects\n",
        "------------------\n",
        "Each row represents a unique project.\n",
        "• project_id - Primary Key  \n",
        "• project_name - Project title  \n",
        "• client_id - FK → clients  \n",
        "• start_date / end_date - Project timeline  \n",
        "• project_manager_id - FK → employees  \n",
        "\n",
        "TABLE 2 - clients\n",
        "-----------------\n",
        "Each row describes a client organization.\n",
        "• client_id - Primary Key  \n",
        "• client_name - Client name  \n",
        "• industry - Sector  \n",
        "• contact_email - Contact address  \n",
        "\n",
        "TABLE 3 - employees\n",
        "-------------------\n",
        "Each row represents a staff member.\n",
        "• employee_id - Primary Key  \n",
        "• name - Full name  \n",
        "• role - Title (Project Manager, Engineer, etc.)  \n",
        "• department - Discipline  \n",
        "• email - Work email  \n",
        "\n",
        "TABLE 4 - project_tasks\n",
        "-----------------------\n",
        "Each row represents a task assigned under a project.\n",
        "• task_id - Primary Key  \n",
        "• project_id - FK → projects  \n",
        "• task_name - Task description  \n",
        "• assigned_to - FK → employees  \n",
        "• start_date / end_date - Task timeline  \n",
        "• status - Task state (Completed, In Progress …)  \n",
        "\n",
        "TABLE 5 - invoices\n",
        "------------------\n",
        "Each row represents a financial record.\n",
        "• invoice_id - Primary Key  \n",
        "• project_id - FK → projects  \n",
        "• amount - Invoice value  \n",
        "• issue_date / due_date - Billing dates  \n",
        "• status - Invoice state (Paid, Pending, Draft …)  \n",
        "\n",
        "RELATIONSHIPS\n",
        "--------------\n",
        "• One client → many projects  \n",
        "• One project → one project manager (employee)  \n",
        "• One project → many tasks and invoices  \n",
        "• Tasks → employees through assigned_to  \n",
        "\n",
        "This structure allows multidimensional queries such as revenue by client, overdue projects, task completion rates, or manager performance.\n",
        "\"\"\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "019889eb",
      "metadata": {},
      "outputs": [],
      "source": [
        "# You can check if the instructions have been sucessfully applied\n",
        "datasource.get_configuration()[\"additional_instructions\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea8f5505-8214-4040-b52d-9525be49e0f4",
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        }
      },
      "outputs": [],
      "source": [
        "# Update or specify (if done for the first time) the example queries. Below is a sample...\n",
        "json_key_pairs_dict = {\n",
        "    \"List all projects.\": \"SELECT project_id, project_name, start_date, end_date FROM projects;\",\n",
        "    \"Show all clients and their industries.\": \"SELECT client_name, industry FROM clients;\",\n",
        "    \"Which employees are project managers?\": \"SELECT name, department FROM employees WHERE role = 'Project Manager';\",\n",
        "    \"List all projects along with their client names.\": \"SELECT p.project_name, c.client_name FROM projects p JOIN clients c ON p.client_id = c.client_id;\",\n",
        "    \"Show all projects managed by 'Ben Li'.\": \"SELECT p.project_name, c.client_name, p.start_date, p.end_date FROM projects p JOIN employees e ON p.project_manager_id = e.employee_id JOIN clients c ON p.client_id = c.client_id WHERE e.name = 'Ben Li';\",\n",
        "    \"List all tasks for the project 'Bridge Strength Assessment'.\": \"SELECT t.task_name, e.name AS assigned_to, t.status FROM project_tasks t JOIN projects p ON t.project_id = p.project_id JOIN employees e ON t.assigned_to = e.employee_id WHERE p.project_name = 'Bridge Strength Assessment';\",\n",
        "    \"Show total invoiced amount per project.\": \"SELECT p.project_name, SUM(i.amount) AS total_invoiced FROM projects p JOIN invoices i ON p.project_id = i.project_id GROUP BY p.project_name;\",\n",
        "    \"Which clients have the highest total billed amount?\": \"SELECT c.client_name, SUM(i.amount) AS total_billed FROM clients c JOIN projects p ON c.client_id = p.client_id JOIN invoices i ON p.project_id = i.project_id GROUP BY c.client_name ORDER BY total_billed DESC;\",\n",
        "    \"How many tasks are still in progress for each project?\": \"SELECT p.project_name, COUNT(*) AS tasks_in_progress FROM projects p JOIN project_tasks t ON p.project_id = t.project_id WHERE t.status = 'In Progress' GROUP BY p.project_name;\",\n",
        "    \"List all pending invoices with project manager and client details.\": \"SELECT i.invoice_id, i.amount, p.project_name, e.name AS project_manager, c.client_name FROM invoices i JOIN projects p ON i.project_id = p.project_id JOIN employees e ON p.project_manager_id = e.employee_id JOIN clients c ON p.client_id = c.client_id WHERE i.status = 'Pending';\",\n",
        "    \"Which projects are overdue (end date before today)?\": \"SELECT p.project_name, c.client_name, p.end_date FROM projects p JOIN clients c ON p.client_id = c.client_id WHERE p.end_date < DATE('now');\",\n",
        "    \"For each project manager, show the number of projects they manage and the total invoice amount.\": \"SELECT e.name AS project_manager, COUNT(DISTINCT p.project_id) AS num_projects, SUM(i.amount) AS total_invoiced FROM employees e JOIN projects p ON e.employee_id = p.project_manager_id JOIN invoices i ON p.project_id = i.project_id GROUP BY e.name;\",\n",
        "    \"Show all tasks completed by employees in the Civil Engineering department.\": \"SELECT t.task_name, p.project_name, e.name AS engineer_name FROM project_tasks t JOIN projects p ON t.project_id = p.project_id JOIN employees e ON t.assigned_to = e.employee_id WHERE e.department = 'Civil Engineering' AND t.status = 'Completed';\",\n",
        "    \"Find clients with projects that have more than 2 pending invoices.\": \"SELECT c.client_name, COUNT(i.invoice_id) AS pending_invoices FROM clients c JOIN projects p ON c.client_id = p.client_id JOIN invoices i ON p.project_id = i.project_id WHERE i.status = 'Pending' GROUP BY c.client_name HAVING COUNT(i.invoice_id) > 2;\"\n",
        "}\n",
        "datasource.add_fewshots(json_key_pairs_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf4940ae-ee7d-437d-8b8a-23a4e6f2e288",
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        }
      },
      "outputs": [],
      "source": [
        "# You can check if the example queries have been sucessfully applied\n",
        "datasource.get_fewshots()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9557f19d",
      "metadata": {},
      "source": [
        "## Publishing Agent\n",
        "Most important step.... You will not be able to test or call agent if not published..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a218cbb3",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Publishing Agent\n",
        "data_agent.publish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ecf0b8f2-5a85-404a-892a-9ba6a356c00b",
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        }
      },
      "outputs": [],
      "source": [
        "# # delete data agent\n",
        "# delete_data_agent(data_agent_name)"
      ]
    }
  ],
  "metadata": {
    "dependencies": {
      "lakehouse": {
        "default_lakehouse": "123aafb5-2b8b-405f-b84d-b60a6e8df469",
        "default_lakehouse_name": "data_agent_lh",
        "default_lakehouse_workspace_id": "c7d1ecc1-d8fa-477c-baf1-47528abf9fe5",
        "known_lakehouses": [
          {
            "id": "123aafb5-2b8b-405f-b84d-b60a6e8df469"
          }
        ]
      }
    },
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "kernelspec": {
      "display_name": "synapse_pyspark",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    },
    "microsoft": {
      "language": "python",
      "language_group": "synapse_pyspark",
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "spark_compute": {
      "compute_id": "/trident/default",
      "session_options": {
        "conf": {
          "spark.synapse.nbs.session.timeout": "1200000"
        }
      }
    },
    "widgets": {}
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
